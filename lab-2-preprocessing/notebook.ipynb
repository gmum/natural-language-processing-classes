{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-2-preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-2-preprocessing/notebook.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RPP549L7OSGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture 2 - Text preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "wPq6nNVXRFos",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example of preprocessing\n",
        "\n",
        "(from [article](https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html) by Matthew Mayo)\n",
        "\n",
        "Beyond the standard Python libraries, we are also using the following:\n",
        "\n",
        "- [NLTK](http://www.nltk.org/) - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, to part of speech tagging, and beyond\n",
        "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
        "- [Inflect](https://pypi.org/project/inflect/) - This is a simple library for accomplishing the natural language related tasks of generating plurals, singular nouns, ordinals, and indefinite articles, and converting numbers to words"
      ]
    },
    {
      "metadata": {
        "id": "gpOQNWkoM83V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "635d0d68-c269-4844-e59d-e7b4a90d973b"
      },
      "cell_type": "code",
      "source": [
        "import re, string, unicodedata\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, PorterStemmer,WordNetLemmatizer\n",
        "!pip install inflect\n",
        "import inflect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting inflect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/1b/6b9b48323b714b5f66dbea2bd5d4166c4f99d908bc31d5307d14083aa9a2/inflect-1.0.1-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hInstalling collected packages: inflect\n",
            "Successfully installed inflect-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rolai6rhTXkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step."
      ]
    },
    {
      "metadata": {
        "id": "mETl7nIUPEIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample = \"\"\"<h1>Title Goes Here</h1>\n",
        "\n",
        "<b>Bolded Text</b>\n",
        "<i>Italicized Text</i>\n",
        "\n",
        "<img src=\"this should all be gone\"/>\n",
        "\n",
        "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
        "\n",
        "I run. He ran. She is running. Will they stop running?\n",
        "\n",
        "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
        "\n",
        "[Some text we don't want to keep is in here]\n",
        "\n",
        "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
        "\n",
        "something... is! wrong() with.,; this :: sentence.\n",
        "\n",
        "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
        "\n",
        "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
        "\n",
        "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
        "\n",
        "[This is some other unwanted text]\n",
        "\n",
        "John: \"Well, well, well.\"\n",
        "James: \"There, there. There, there.\"\n",
        "\n",
        "&nbsp;&nbsp;\n",
        "\n",
        "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
        "I have to go get 2 tutus from 2 different stores, too.\n",
        "\n",
        "22    45   1067   445\n",
        "\n",
        "{{Here is some stuff inside of double curly braces.}}\n",
        "{Here is more stuff in single curly braces.}\n",
        "\n",
        "[DELETE]\n",
        "\n",
        "</body>\n",
        "</html>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JIq4ucjJTZOm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A toy dataset indeed, but make no mistake; the steps we are taking here to preprocessing this data are fully transferable.\n",
        "\n",
        "The text data preprocessing framework:\n",
        "\n",
        "![](https://www.kdnuggets.com/wp-content/uploads/text-preprocessing-framework-2.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "I_PQ5VcpUULr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Noise Removal\n",
        "\n",
        "Let's loosely define noise removal as text-specific normalization tasks which often take place prior to tokenization. Some would argue that, while the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.\n",
        "\n",
        "Sample noise removal tasks could include:\n",
        "\n",
        "- removing text file headers, footers\n",
        "- removing HTML, XML, etc. markup and metadata\n",
        "- extracting valuable data from other formats, such as JSON\n",
        "\n",
        "As you can imagine, the boundary between noise removal and data collection and assembly, on the one hand, is a fuzzy one, while the line between noise removal and normalization is blurred on the other. Given its close relationship with specific texts and their collection and assembly, many denoising tasks, such as parsing a JSON structure, would obviously need to be implemented prior to tokenization.\n",
        "\n",
        "In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library, and use regular expressions to remove open and close double brackets and anything in between them (we assume this is necessary based on our sample text)."
      ]
    },
    {
      "metadata": {
        "id": "bLEWD4nzRfTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "sample = denoise_text(sample)\n",
        "print(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJFXqjZmUvg3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words.\n",
        "\n",
        "For our task, we will tokenize our sample text into a list of words. This is done using NTLK's word_tokenize() function.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XwaEnLwYUSzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sample)\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxP_Pd7rWhtM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Normalization\n",
        " \n",
        "Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
        "\n",
        "Normalizing text can mean performing a number of tasks, but for our framework we will approach normalization in 3 distinct steps: \n",
        "- stemming, \n",
        "- lemmatization,\n",
        "- everything else. \n",
        "\n",
        "For specifics on what these distinct steps may be, [see this post](https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html).\n",
        "\n",
        "Remember, after tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does."
      ]
    },
    {
      "metadata": {
        "id": "FAtYfNqoWOLd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "words = normalize(words)\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUrEKj91ZaPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calling the stemming and lemming functions are done as below:"
      ]
    },
    {
      "metadata": {
        "id": "CINpnkB4YwHY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words)\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return stems, lemmas\n",
        "\n",
        "stems, lemmas = stem_and_lemmatize(words)\n",
        "print('Stemmed:\\n', stems)\n",
        "print('\\nLemmatized:\\n', lemmas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnJWHosGZ7Fe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depending on your NLP task or preference, one of these may be more appropriate than the other. See here for a [discussion on lemmatization vs stemming](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/)."
      ]
    },
    {
      "metadata": {
        "id": "gW6rm0134_E-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to resolve ambiguous cases, lemmatization usually requires tokens to be accompanied by part-of-speech tags. For example, the word lemma for rose depends on whether it is used as a noun or a verb:"
      ]
    },
    {
      "metadata": {
        "id": "UnsGwUpg5JDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmer = WordNetLemmatizer()\n",
        "print(f\"noun lemmatization: {lemmer.lemmatize('rose', 'n')}\")\n",
        "print(f\"verb lemmatization: {lemmer.lemmatize('rose', 'v')}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUXQmKLG5h_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1."
      ]
    },
    {
      "metadata": {
        "id": "q9CtaF3zZcRR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_1 = \"\"\"Now the Children of Ilu´vatar are Elves and Men, the Firstborn\n",
        "and the Followers. And amid all the splendours of the\n",
        "World, its vast halls and spaces, and its wheeling fires, Ilu´vatar\n",
        "chose a place for their habitation in the Deeps of Time\n",
        "and in the midst of the innumerable stars. And this habitation\n",
        "might seem a little thing to those who consider only the\n",
        "majesty of the Ainur, and not their terrible sharpness; as who\n",
        "should take the whole field of Arda for the foundation of a\n",
        "pillar and so raise it until the cone of its summit were more\n",
        "bitter than a needle; or who consider only the immeasurable\n",
        "vastness of the World, which still the Ainur are shaping, and\n",
        "not the minute precision to which they shape all things\n",
        "therein. But when the Ainur had beheld this habitation in a\n",
        "vision and had seen the Children of Ilu´vatar arise therein,\n",
        "then many of the most mighty among them bent all their\n",
        "thought and their desire towards that place. And of these\n",
        "Melkor was the chief, even as he was in the beginning the\n",
        "greatest of the Ainur who took part in the Music. And he\n",
        "feigned, even to himself at first, that he desired to go thither\n",
        "and order all things for the good of the Children of Ilu´vatar,\n",
        "controlling the turmoils of the heat and the cold that had\n",
        "come to pass through him. But he desired rather to subdue\n",
        "to his will both Elves and Men, envying the gifts with which\n",
        "Ilu´vatar promised to endow them; and he wished himself to\n",
        "have subjects and servants, and to be called Lord, and to be\n",
        "a master over other wills.\n",
        "But the other Ainur looked upon this habitation set within\n",
        "the vast spaces of the World, which the Elves call Arda,\n",
        "the Earth; and their hearts rejoiced in light, and their eyes\n",
        "beholding many colours were filled with gladness; but\n",
        "because of the roaring of the sea they felt a great unquiet.\n",
        "And they observed the winds and the air, and the matters of\n",
        "which Arda was made, of iron and stone and silver and gold\n",
        "and many substances: but of all these water they most greatly\n",
        "praised. And it is said by the Eldar that in water there lives\n",
        "yet the echo of the Music of the Ainur more than in any\n",
        "substance else that is in this Earth; and many of the Children\n",
        "of Ilu´vatar hearken still unsated to the voices of the Sea, and\n",
        "yet know not for what they listen.\n",
        "Now to water had that Ainu whom the Elves call Ulmo\n",
        "turned his thought, and of all most deeply was he instructed\n",
        "by Ilu´vatar in music. But of the airs and winds Manwe¨ most\n",
        "had pondered, who is the noblest of the Ainur. Of the fabric\n",
        "of Earth had Aule¨ thought, to whom Ilu´vatar had given skill\n",
        "and knowledge scare less than to Melkor; but the delight and\n",
        "pride of Aule¨ is in the deed of making, and in the thing made,\n",
        "and neither in possession nor in his own mastery; wherefore\n",
        "he gives and hoards not, and is free from care, passing ever\n",
        "on to some new work.\n",
        "And Ilu´vatar spoke to Ulmo, and said: ‘Seest thou not how\n",
        "here in this little realm in the Deeps of Time Melkor hath\n",
        "made war upon thy province? He hath bethought him of\n",
        "bitter cold immoderate, and yet hath not destroyed the beauty\n",
        "of thy fountains, nor of thy clear pools. Behold the snow,\n",
        "and the cunning work of frost! Melkor hath devised heats\n",
        "and fire without restraint, and hath not dried up thy desire\n",
        "nor utterly quelled the music of the sea. Behold rather the\n",
        "height and glory of the clouds, and the everchanging mists;\n",
        "and listen to the fall of rain upon the Earth! And in these\n",
        "clouds thou art drawn nearer to Manwe¨, thy friend, whom\n",
        "thou lovest.’\n",
        "Then Ulmo answered: ‘Truly, Water is become now fairer\n",
        "than my heart imagined, neither had my secret thought conceived\n",
        "the snowflake, nor in all my music was contained the\n",
        "falling of the rain. I will seek Manwe¨, that he and I may make\n",
        "melodies for ever to thy delight!’ And Manwe¨ and Ulmo have\n",
        "from the beginning been allied, and in all things have served\n",
        "most faithfully the purpose of Ilu´vatar.\n",
        "But even as Ulmo spoke, and while the Ainur were yet\n",
        "gazing upon this vision, it was taken away and hidden from\n",
        "their sight; and it seemed to them that in that moment they\n",
        "perceived a new thing, Darkness, which they had not known\n",
        "before except in thought. But they had become enamoured\n",
        "of the beauty of the vision and engrossed in the unfolding\n",
        "of the World which came there to being, and their minds\n",
        "were filled with it; for the history was incomplete and the\n",
        "circles of time not full-wrought when the vision was taken\n",
        "away. And some have said that the vision ceased ere the\n",
        "fulfilment of the Dominion of Men and the fading of the\n",
        "Firstborn; wherefore, though the Music is over all, the Valar\n",
        "have not seen as with sight the Later Ages or the ending of\n",
        "the World.\n",
        "Then there was unrest among the Ainur; but Ilu´vatar called\n",
        "to them, and said: ‘I know the desire of your minds that what\n",
        "ye have seen should verily be, not only in your thought, but\n",
        "even as ye yourselves are, and yet other. Therefore I say: Ea¨!\n",
        "Let these things Be! And I will send forth into the Void the\n",
        "Flame Imperishable, and it shall be at the heart of the World,\n",
        "and the World shall Be; and those of you that will may go\n",
        "down into it.’ And suddenly the Ainur saw afar off a light,\n",
        "as it were a cloud with a living heart of flame; and they knew\n",
        "that this was no vision only, but that Ilu´vatar had made a\n",
        "new thing: Ea¨, the World that Is.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tb2vn6GldyWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Make a vocabulary that for each token contains the number of its occurencies in above text. Store the vocabulary as a list of tuples. Sort this vocabulary by the number of occurences, from biggest to smallest.  Use word_tokenize from nltk.\n",
        "\n",
        "2. Repeat this process, but this time also convert all tokens to lowercase and lemmatize all tokens as verbs.\n",
        "\n",
        "3. Use nltk.sent_tokenize to find the longest sentence (with respect to number of characters) in the text and return the number of words in this sentence (excluding punctuation!)\n",
        "\n",
        "4. Read about [different tokenizers](https://www.nltk.org/api/nltk.tokenize.html) from NLTK. Give example of sentence, that would be tokenized better by TweetTokenizer().tokenize() and a sentence that would be better after word_tokenize().\n",
        "\n",
        "\n",
        "Hints:\n",
        "- you might need to deal with \\n after each line, it can be converted to a space\n"
      ]
    },
    {
      "metadata": {
        "id": "57F0W2O4jzZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#solution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jT6qOuLnj0LL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1\n",
        "vocab_1 = \n",
        "\n",
        "# 2\n",
        "vocab_2 = \n",
        "\n",
        "# 3\n",
        "num_tokens = \n",
        "\n",
        "\n",
        "assert len(vocab_1) == 379\n",
        "assert len(vocab_2) == 336\n",
        "assert num_tokens == 82"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tY477m0kxvqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Z77h33bpkxJu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw = nltk.corpus.gutenberg.raw(\"burgess-busterbrown.txt\")\n",
        "# print(raw[:500])\n",
        "words = nltk.corpus.gutenberg.words(\"burgess-busterbrown.txt\")\n",
        "# print(words[:20])\n",
        "sents = nltk.corpus.gutenberg.sents(\"burgess-busterbrown.txt\")\n",
        "# print(sents[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PthHXjgcvqa5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using \"burgess-busterbrown.txt\", do the following:\n",
        "\n",
        "1. Count the number of sentences containing token \"the\" (case insensitive)\n",
        "2. Compute the average **token** length in the above corpus.\n",
        "3. (Stemming) Find tokens from above file that differ after using [Porter](http://snowball.tartarus.org/algorithms/english/stemmer.html) and [Lancaster](https://www.nltk.org/_modules/nltk/stem/lancaster.html) stemming algorithms (after lowercasing the tokens).\n",
        "4. (Lemmatization) Perform lemmatization on above corpus. Use POS tagger (defined below) to improve the lemmatizer. Give an example (from corpus) where using POS tagger helps."
      ]
    },
    {
      "metadata": {
        "id": "9ViiHVVHwClI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#solution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHK4Q6l6wQVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For 4:\n",
        "\n",
        "def pt_to_wn(pos):\n",
        "    \"\"\"\n",
        "    Takes a Penn Treebank tag and converts it to an\n",
        "    appropriate WordNet equivalent for lemmatization.\n",
        "\n",
        "    A list of Penn Treebank tags is available at:\n",
        "    https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "    \"\"\"\n",
        "\n",
        "    from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "\n",
        "    pos = pos.lower()\n",
        "\n",
        "    if pos.startswith('jj'):\n",
        "        tag = ADJ\n",
        "    elif pos == 'md':\n",
        "        # Modal auxiliary verbs\n",
        "        tag = VERB\n",
        "    elif pos.startswith('rb'):\n",
        "        tag = ADV\n",
        "    elif pos.startswith('vb'):\n",
        "        tag = VERB\n",
        "    elif pos == 'wrb':\n",
        "        # Wh-adverb (how, however, whence, whenever...)\n",
        "        tag = ADV\n",
        "    else:\n",
        "        # default to VERB\n",
        "        # This is not strictly correct, but it is good\n",
        "        # enough for lemmatization.\n",
        "        tag = VERB\n",
        "\n",
        "    return tag\n",
        "  \n",
        "  \n",
        "def nltk_pos_tagger(tokens):\n",
        "    \"\"\"\n",
        "    Takes a list of tokens and returns a list of \n",
        "    tuples [(token, wordnet_tag), ..]\n",
        "    \"\"\"\n",
        "\n",
        "    # Tag tokens with part-of-speech:\n",
        "    tagged = nltk.pos_tag(tokens) \n",
        "\n",
        "    # Convert our Treebank-style tags to WordNet-style tags.\n",
        "    tagged = [(word, pt_to_wn(tag))\n",
        "                     for (word, tag) in tagged]\n",
        "    return tagged\n",
        "  \n",
        "  \n",
        "def lemmatizer(tokens):\n",
        "    tagged = nltk_pos_tagger(tokens)  \n",
        "    \n",
        "    #write code to lemmatize tokens using taggs from nltk_pos_tagger\n",
        "    \n",
        "    pass\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJ2a2JBmHZ8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}