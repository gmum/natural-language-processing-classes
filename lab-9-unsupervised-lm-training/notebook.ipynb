{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-9-unsupervised-lm-training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-9-unsupervised-lm-training/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Xl-tY1QsFKex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 9 - Training unsupervised Language Models\n",
        "\n",
        "## Excercise (1 pt)\n",
        "\n",
        "Fill the gap in the training loop and eval loop in below code and train an Language Model that obtains at least **111** perplexity on the test set.\n",
        "\n",
        "Remember to:\n",
        "-  use gradient clipping with value 0.25\n",
        "-  use hidden state from previous batch in next batch, to keep the information longer. To do this, instead of initializing the hidden state with 0 each batch, we detach the hidden state from how it was previously produced. If we didn't, the model would try backpropagating all the way to start of the dataset. Use repackage_hidden to deal with that problem."
      ]
    },
    {
      "metadata": {
        "id": "4UWE49iajKZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77285be4-5bcc-449c-ec68-e7e9d493d1ab"
      },
      "cell_type": "code",
      "source": [
        "# Use GPU support!\n",
        "\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Use dataset from https://drive.google.com/drive/folders/1e-BUHYY61Vy9AGNuh2nungslO-mYuVox?usp=sharing\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u2cnfYyip3Mj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "    \"\"\"Build word2idx and idx2word from Corpus(train/val/test)\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {} # word: index\n",
        "        self.idx2word = [] # position(index): word\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"Create/Update word2idx and idx2word\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    \"\"\"Corpus Tokenizer\"\"\"\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'ptb.valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'ptb.test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                # line to list of token + eos\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids\n",
        "      \n",
        "      \n",
        "def batchify(data, bsz, verbose=False):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    # See https://pytorch.org/docs/stable/torch.html#torch.narrow for more explaination\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    # .t() is transposition: https://pytorch.org/docs/stable/torch.html#torch.t\n",
        "    # the contiguous function doesn't affect your target tensor at all, it just \n",
        "    # makes sure that it is stored in a contiguous chunk of memory.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    if verbose:\n",
        "      print(data.size())\n",
        "      for el in data[:50,0]:\n",
        "        print(corpus.dictionary.idx2word[el.item()])\n",
        "      \n",
        "    data = data.cuda()\n",
        "    return data\n",
        "\n",
        "# use path to where you store the datasets\n",
        "corpus = Corpus('/content/gdrive/My Drive/nlp-classes/labs/lab-9')\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size, verbose=True)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "ntokens = len(corpus.dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IriDBdrYrh40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        return (torch.zeros(self.nlayers, bsz, self.nhid).cuda(),\n",
        "                torch.zeros(self.nlayers, bsz, self.nhid).cuda())\n",
        "\n",
        "model = LSTMModel(ntokens, 150, 150, 1, 0.2)\n",
        "model.cuda()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4pz0dFN00-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.cuda()\n",
        "seq_len = 30\n",
        "log_interval = 100\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "    if type(h) == torch.Tensor:\n",
        "        return Variable(h.data)\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def get_batch(source, i):\n",
        "    s_len = min(seq_len, len(source) - 1 - i)\n",
        "    data = Variable(source[i:i+s_len])\n",
        "    target = Variable(source[i+1:i+1+s_len].view(-1))\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    \"\"\"compute total loss on data_source dataset\"\"\"\n",
        "  \n",
        "    model.eval() # Turn on evaluation mode which disables dropout.\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "      \n",
        "      \n",
        "      \n",
        "        \"\"\" Write your code here \"\"\"\n",
        "      \n",
        "      \n",
        "      \n",
        "    return total_loss[0] / len(data_source)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "        \"\"\" Write your code here \"\"\"\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss[0] / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // seq_len, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "      epoch_start_time = time.time()\n",
        "      train()\n",
        "      val_loss = evaluate(val_data)\n",
        "      print('-' * 89)\n",
        "      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         val_loss, math.exp(val_loss)))\n",
        "      print('-' * 89)\n",
        "\n",
        "      # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "      if not best_val_loss or val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "      else:\n",
        "          lr /= 4.0\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}