{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-6-word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-6-word2vec/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zfeZuPw4dW5j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 6 - word 2 vec\n",
        "\n",
        " \"A word is characterized by the company it keeps\" - Firth (1957)"
      ]
    },
    {
      "metadata": {
        "id": "Au1y1zTzg5L1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise (2pt)\n",
        "\n",
        "- Implement word2vec on harry potter corpus. Use skip-gram model, assume context window = 5. Use negative sampling.\n",
        "- Remember that preprocessing might help\n",
        "- Define a measure of similarity between two vectors (https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "- Compute:\n",
        "  - $cosine\\_similarity(vec['harry'], vec['he']), cosine\\_similarity(vec['harry'], vec['she'])$  \n",
        "  \n",
        "  - $cosine\\_similarity(vec['voldemort'], vec['he']), cosine\\_similarity(vec['voldemort'], vec['she'])$  \n",
        "  \n",
        "  - $cosine\\_similarity(vec['hermione'], vec['she']), cosine\\_similarity(vec['hermione'], vec['he'])$  \n",
        " \n",
        "  - $torch.dot(vec['harry'], vec['he']), torch.dot(vec['harry'], vec['she'])$  \n",
        "  \n",
        "  - $torch.dot(vec['voldemort'], vec['he']), torch.dot(vec['voldemort'], vec['she'])$  \n",
        "  \n",
        "  - $torch.dot(vec['hermione'], vec['she']), torch.dot(vec['hermione'], vec['he'])$  \n",
        "\n",
        "Worth reading:\n",
        "- notes from the lecture :)\n",
        "- https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb - introduction to word2vec in pytorch (no negative sampling here)\n",
        "- https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08 - negative sampling, hierarchical softmax"
      ]
    },
    {
      "metadata": {
        "id": "VOKM7MX0g779",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-Fx_0DGjZDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download the folder to your drive and paste the path to your folder below\n",
        "path_to_folder = '/content/gdrive/My Drive/nlp-classes/labs/lab-5/' # this should be path to the folder in your drive\n",
        "\n",
        "with open(os.path.join(path_to_folder, 'hp.txt'), 'r') as f:\n",
        "  hp = f.read().replace('\\n', ' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}