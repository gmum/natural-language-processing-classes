{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-6-word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-6-word2vec/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zfeZuPw4dW5j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 6 - word-2-vec with pytorch and gensim\n",
        "\n",
        " \"A word is characterized by the company it keeps\" - Firth (1957)\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "Au1y1zTzg5L1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise 1 (2pt)\n",
        "\n",
        "\n",
        "- Train word2vec skip-gram model on sentence \"the quick brown fox jumps over the lazy dog\". Assume context window = 2, embedding_dim = 5. No preprocessing apart from tokenization.\n",
        "- Compute model output probabilities for words \"lazy\" and \"dog\". If you have trained the model correctly, the output probabilities for word \"lazy\" should be higher for words \"over\", \"the\", \"dog\" (close to 1/3 each) and lower for other words (close to 0 each). For word \"dog\", the output probabilities should be higher for words, \"the\", \"dog\" (close to 1/2 each) and lower for other words (close to 0 each). \n",
        "- Compute dot product between the vector of word \"dog\" and the vector of word \"lazy\" (could be representation of center vector and representation of context vector) and between \"dog\" and \"brown\". Which one is higher? Why?\n",
        "\n",
        "\n",
        "You can use this tutorial https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
        "\n",
        "Use pytorch (or tensorflow)."
      ]
    },
    {
      "metadata": {
        "id": "VOKM7MX0g779",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-Fx_0DGjZDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = \"the quick brown fox jumps over the lazy dog\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CRnIu7hAB6sF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If our vocabulary is bigger, the word2vec model needs a LOT of data to obtain reasonable results. With this amount of data, the code needs to be optimized very well. Writing such code will be more suitable for a project instead of a simple exercise, therefore in the next exercise we will use [gensim](https://radimrehurek.com/gensim/), a library made for efficient training of word vectors.\n",
        "\n",
        "# * Exercise 2 (2pt)\n",
        "\n",
        "- Use [gensim](https://radimrehurek.com/gensim/) to train a word2vec model on [OpinRank](http://kavita-ganesan.com/entity-ranking-data/). You can follow this [tutorial](https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3), but make sure you have used negative sampling.\n",
        "- Find 10 similar words to word \"dirty\" and \"canada\"\n",
        "- Check if similarity between \"dirty\" and \"dusty\" is bigger than between \"dirty\" and \"unclean\""
      ]
    },
    {
      "metadata": {
        "id": "m-WqD1xKDE0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}